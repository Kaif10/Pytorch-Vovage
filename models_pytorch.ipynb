{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PYTORCH basic model building practice"
      ],
      "metadata": {
        "id": "oGLAL4_oAeAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "2nTz9yO2AgRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "S_xkDRW9ynFp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fully connected layer\n",
        "\n",
        "Every input influences every output of the layer to a degree specified by the layer’s weight\n",
        "\n",
        "If you do the matrix multiplication of x by the linear layer’s weights, and add the biases, you’ll find that you get the output vector y.\n",
        "\n",
        "One other important feature to note: When we checked the weights of our layer with lin.weight, it reported itself as a Parameter (which is a subclass of Tensor), and let us know that it’s tracking gradients with autograd. This is a default behavior for Parameter that differs from Tensor"
      ],
      "metadata": {
        "id": "DzQOiPLACbek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JiacYfUC6sRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1336575-39c9-4dff-90e2-272d46eaf358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 0.3160,  0.0632,  0.3253],\n",
            "        [ 0.3683, -0.2402, -0.0110]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.4375, -0.1672], requires_grad=True)\n",
            "Input:\n",
            "tensor([[0.9866, 0.9736, 0.7939]])\n",
            "\n",
            "\n",
            "Output:\n",
            "tensor([[ 1.0689, -0.0464]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fundamental structure of a PyTorch model: \n",
        "\n",
        "there is an __init__() method that defines the layers and other components of a model, and a forward() method where the computation gets done."
      ],
      "metadata": {
        "id": "L5tu-wOcCHWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "   \n",
        "   # The first thing is writing an __init__ function that references nn.Module and inherits its properties\n",
        "   # define the layers in your neural network in this function\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    \n",
        "    # When you use PyTorch to build a model, you just have to define the forward function, \n",
        "    # that will pass the data into the computation graph (i.e. our neural network). \n",
        "    # This will represent our feed-forward algorithm.\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n"
      ],
      "metadata": {
        "id": "ckTbdsLBypcV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer i.e layer 2:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer 2 params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5rMrDKT6ZKR",
        "outputId": "fb1a7a1a-cdac-4fe0-8a05-01b8e4c4bcb3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer i.e layer 2:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Model params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0735, -0.0662, -0.0192,  ...,  0.0976,  0.0450,  0.0197],\n",
            "        [-0.0040, -0.0960,  0.0430,  ...,  0.0074,  0.0377, -0.0826],\n",
            "        [ 0.0905, -0.0177, -0.0497,  ..., -0.0517,  0.0331,  0.0070],\n",
            "        ...,\n",
            "        [-0.0394, -0.0480, -0.0800,  ..., -0.0285, -0.0301,  0.0750],\n",
            "        [-0.0744, -0.0153, -0.0002,  ..., -0.0981, -0.0740, -0.0560],\n",
            "        [-0.0310, -0.0331, -0.0480,  ..., -0.0365,  0.0626, -0.0377]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0092,  0.0966, -0.0725,  0.0587,  0.0363,  0.0189,  0.0630, -0.0194,\n",
            "        -0.0760,  0.0195,  0.0080, -0.0151, -0.0947,  0.0791, -0.0139,  0.0985,\n",
            "         0.0218,  0.0876, -0.0711,  0.0433, -0.0066,  0.0172,  0.0178, -0.0890,\n",
            "        -0.0730, -0.0805, -0.0182,  0.0770, -0.0102,  0.0264, -0.0253,  0.0560,\n",
            "        -0.0372, -0.0070,  0.0614,  0.0163,  0.0448,  0.0976, -0.0381,  0.0707,\n",
            "         0.0855,  0.0046, -0.0112,  0.0857,  0.0606, -0.0661, -0.0985, -0.0282,\n",
            "        -0.0406, -0.0227, -0.0093,  0.0727, -0.0601, -0.0656,  0.0857, -0.0970,\n",
            "        -0.0318, -0.0839,  0.0517, -0.0630,  0.0889, -0.0156, -0.0828, -0.0568,\n",
            "        -0.0403, -0.0172, -0.0668,  0.0740,  0.0683,  0.0839, -0.0805,  0.0070,\n",
            "         0.0878, -0.0165, -0.0574, -0.0757, -0.0915, -0.0811,  0.0910,  0.0159,\n",
            "        -0.0463, -0.0487,  0.0527, -0.0229,  0.0168,  0.0329,  0.0633, -0.0903,\n",
            "        -0.0847,  0.0151,  0.0049, -0.0052,  0.0549, -0.0035,  0.0823,  0.0350,\n",
            "         0.0601, -0.0150,  0.0415, -0.0174, -0.0400,  0.0873,  0.0867,  0.0846,\n",
            "         0.0583,  0.0675,  0.0259, -0.0370,  0.0486, -0.0078,  0.0015,  0.0684,\n",
            "        -0.0861,  0.0902,  0.0493, -0.0751, -0.0798, -0.0571,  0.0260,  0.0094,\n",
            "         0.0422,  0.0664,  0.0275, -0.0090,  0.0965,  0.0036,  0.0943,  0.0221,\n",
            "         0.0946,  0.0172, -0.0934,  0.0708, -0.0323, -0.0568,  0.0508,  0.0151,\n",
            "        -0.0653, -0.0880, -0.0960, -0.0205, -0.0308, -0.0425, -0.0959, -0.0033,\n",
            "         0.0695, -0.0194, -0.0507,  0.0573, -0.0948,  0.0922,  0.0859,  0.0327,\n",
            "         0.0035,  0.0915,  0.0308,  0.0286,  0.0290, -0.0534, -0.0193,  0.0280,\n",
            "         0.0598,  0.0777, -0.0358,  0.0889,  0.0984, -0.0443, -0.0871, -0.0790,\n",
            "         0.0570, -0.0011, -0.0217,  0.0043,  0.0390,  0.0257,  0.0747, -0.0943,\n",
            "        -0.0303, -0.0772, -0.0047, -0.0798, -0.0923, -0.0076,  0.0147, -0.0815,\n",
            "         0.0118, -0.0088, -0.0311, -0.0959, -0.0136, -0.0128,  0.0754,  0.0618,\n",
            "         0.0837,  0.0817,  0.0018,  0.0095,  0.0906,  0.0668, -0.0723, -0.0593],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0095, -0.0129,  0.0172,  ...,  0.0166, -0.0697, -0.0219],\n",
            "        [-0.0222, -0.0272, -0.0072,  ..., -0.0590, -0.0343,  0.0652],\n",
            "        [ 0.0544, -0.0223,  0.0054,  ...,  0.0211, -0.0634,  0.0502],\n",
            "        ...,\n",
            "        [-0.0256, -0.0404, -0.0704,  ...,  0.0476,  0.0263, -0.0382],\n",
            "        [-0.0039, -0.0091, -0.0365,  ...,  0.0548, -0.0267, -0.0171],\n",
            "        [ 0.0210, -0.0034, -0.0635,  ..., -0.0501, -0.0041, -0.0263]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0561,  0.0452, -0.0687,  0.0580,  0.0598,  0.0411, -0.0269,  0.0400,\n",
            "        -0.0093, -0.0065], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer 2 params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0095, -0.0129,  0.0172,  ...,  0.0166, -0.0697, -0.0219],\n",
            "        [-0.0222, -0.0272, -0.0072,  ..., -0.0590, -0.0343,  0.0652],\n",
            "        [ 0.0544, -0.0223,  0.0054,  ...,  0.0211, -0.0634,  0.0502],\n",
            "        ...,\n",
            "        [-0.0256, -0.0404, -0.0704,  ...,  0.0476,  0.0263, -0.0382],\n",
            "        [-0.0039, -0.0091, -0.0365,  ...,  0.0548, -0.0267, -0.0171],\n",
            "        [ 0.0210, -0.0034, -0.0635,  ..., -0.0501, -0.0041, -0.0263]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0561,  0.0452, -0.0687,  0.0580,  0.0598,  0.0411, -0.0269,  0.0400,\n",
            "        -0.0093, -0.0065], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gh_6Lc6uypf2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional layers are built to handle data with a high degree of spatial correlation. They are very commonly used in computer vision, where they detect close groupings of features which the compose into higher-level features."
      ],
      "metadata": {
        "id": "6j3ZlFxlUqXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F\n",
        "\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        #inherit torch.nn.Module properties\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "metadata": {
        "id": "ITISEqXryxGs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first argument to a convolutional layer’s constructor is the number of input channels. Here, it is 1. If we were building this model to look at 3-color channels, it would be 3.\n",
        "\n",
        "A convolutional layer is like a window that scans over the image, looking for a pattern it recognizes. These patterns are called features\n",
        "\n",
        "This is the second argument to the constructor is the number of output features. Here, we’re asking our layer to learn 6 features.\n",
        "\n",
        " The third argument is the window or kernel size. Here, the “5” means we’ve chosen a 5x5 kernel. (If you want a kernel with height different from width, you can specify a tuple for this argument - e.g., (3, 5) to get a 3x5 convolution kernel (window).)"
      ],
      "metadata": {
        "id": "pgeJbZwH2V69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "RFm1aUEx3pek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Network"
      ],
      "metadata": {
        "id": "WjtLzOcnpeZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks (or RNNs) are used for sequential data - anything from time-series measurements from a scientific instrument to natural language sentences to DNA nucleotides. An RNN does this by maintaining a hidden state that acts as a sort of memory for what it has seen in the sequence so far."
      ],
      "metadata": {
        "id": "tLlp2ywQUY_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "FxocbGmn6uoW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constructor has four arguments:\n",
        "\n",
        "vocab_size is the number of words in the input vocabulary. Each word is a one-hot vector (or unit vector) in a vocab_size-dimensional space.\n",
        "\n",
        "tagset_size is the number of tags in the output set.\n",
        "\n",
        "embedding_dim is the size of the embedding space for the vocabulary. An embedding maps a vocabulary onto a low-dimensional space, where words with similar meanings are close together in the space.\n",
        "\n",
        "hidden_dim is the size of the LSTM’s memory.\n",
        "The input will be a sentence with the words represented as indices of one-hot vectors. The embedding layer will then map these down to an embedding_dim-dimensional space. The LSTM takes this sequence of embeddings and iterates over it, fielding an output vector of length hidden_dim. \n",
        "\n",
        "The final linear layer acts as a classifier; applying log_softmax() to the output of the final layer converts the output into a normalized set of estimated probabilities that a given word maps to a given tag."
      ],
      "metadata": {
        "id": "TPcu53aq38lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "ixvvRj3E3-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max Pooling"
      ],
      "metadata": {
        "id": "UKfUEo2QpaqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Max pooling (and its twin, min pooling) reduce a tensor by combining cells, and assigning the maximum value of the input cells to the output cell (we saw this). For example:\n",
        "\n",
        "my_tensor = torch.rand(1, 6, 6)\n",
        "print(my_tensor)\n",
        "\n",
        "# run a stride of 3 and pick the max from each\n",
        "#final will be reduced to (1, 2, 2)\n",
        "maxpool_layer = torch.nn.MaxPool2d(3)\n",
        "print(maxpool_layer(my_tensor))\n",
        "\n",
        "# run a stride of 2 and pick the max from each\n",
        "#final will be reduced to (1, 3, 3)\n",
        "maxpool_layer2 = torch.nn.MaxPool2d(2)\n",
        "print(maxpool_layer2(my_tensor))"
      ],
      "metadata": {
        "id": "qTYGBDcc6ut5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03aae00-d961-4d6c-a597-6267499b8451"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.5793, 0.5834, 0.4753, 0.3510, 0.5525, 0.1278],\n",
            "         [0.8813, 0.9122, 0.3859, 0.3544, 0.0258, 0.2757],\n",
            "         [0.4768, 0.0720, 0.0415, 0.6416, 0.6504, 0.0046],\n",
            "         [0.1300, 0.3771, 0.1513, 0.6127, 0.9971, 0.7925],\n",
            "         [0.0266, 0.4219, 0.2332, 0.9785, 0.4893, 0.7705],\n",
            "         [0.6212, 0.0087, 0.3937, 0.3318, 0.3556, 0.4316]]])\n",
            "tensor([[[0.9122, 0.6504],\n",
            "         [0.6212, 0.9971]]])\n",
            "tensor([[[0.9122, 0.4753, 0.5525],\n",
            "         [0.4768, 0.6416, 0.9971],\n",
            "         [0.6212, 0.9785, 0.7705]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization layer"
      ],
      "metadata": {
        "id": "v622JkIdofH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization layers re-center and normalize the output of one layer before feeding it to another. \n",
        "#Centering the and scaling the intermediate tensors has a number of beneficial effects,  such as letting you use higher learning rates without exploding/vanishing gradients.\n",
        "\n",
        "original_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
        "print(original_tensor)\n",
        "print(original_tensor.mean())\n",
        "\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4)\n",
        "normed_tensor = norm_layer(original_tensor)\n",
        "print(normed_tensor)\n",
        "print(normed_tensor.mean())\n"
      ],
      "metadata": {
        "id": "S675Sx-W6uwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d0d980-2d19-43cd-ecff-327e859cd7c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 8.0735, 15.8688, 18.2773, 24.4952],\n",
            "         [ 9.5099, 10.2522, 23.2088,  6.7036],\n",
            "         [14.2245, 23.6516,  8.1415, 12.5767],\n",
            "         [23.3453,  6.9129, 18.5149, 19.1098]]])\n",
            "tensor(15.1792)\n",
            "tensor([[[-1.4631, -0.1377,  0.2718,  1.3290],\n",
            "         [-0.4567, -0.3402,  1.6942, -0.8974],\n",
            "         [-0.0750,  1.5923, -1.1509, -0.3664],\n",
            "         [ 1.0453, -1.6493,  0.2532,  0.3508]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "tensor(1.8626e-08, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout Layers"
      ],
      "metadata": {
        "id": "k3hflOHGpMe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropout layers are a tool for encouraging sparse representations in your model - that is, pushing it to do inference with less data.\n",
        "\n",
        "#Dropout layers work by randomly setting parts of the input tensor during training - dropout layers are always turned off for inference.\n",
        "# This forces the model to learn against this masked or reduced dataset. For example:\n",
        "\n",
        "my_tensor = torch.rand(1, 4, 4)\n",
        "print(my_tensor)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4)\n",
        "print(dropout(my_tensor))\n"
      ],
      "metadata": {
        "id": "jN1yOGkj6u0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a4c39f-93d6-46fe-a1c7-d6d0cb026d76"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.1194, 0.7049, 0.2922, 0.4484],\n",
            "         [0.9159, 0.4119, 0.2081, 0.0032],\n",
            "         [0.5486, 0.8726, 0.4754, 0.4427],\n",
            "         [0.1700, 0.8789, 0.0528, 0.9664]]])\n",
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.5265, 0.6866, 0.0000, 0.0053],\n",
            "         [0.9143, 0.0000, 0.7923, 0.0000],\n",
            "         [0.0000, 1.4649, 0.0000, 1.6106]]])\n"
          ]
        }
      ]
    }
  ]
}